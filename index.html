<!DOCTYPE html>
<html lang="en">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <title>Yiqi Wang</title>

    <meta name="author" content="Yiqi Wang">
    <meta name="viewport" content="width=900">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="icon" type="image/png" href="images/cmulogo.png">
  
    <!-- <script src=”main.js” defer></script> -->

</head>


<body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
            <td width="70%" valign="left">
                <p style="text-align:center">
                    <name>Yiqi Wang</name>
                </p>
                <p>
                I am an incoming master student in robotics (MSR) at Robotics Institute, Carnegie Mellon University (CMU), working with Prof. Aviral Kumar. Previously, I am an ECE master student (CMU) worked with Prof. <a href="https://users.ece.cmu.edu/~yuejiec/">Yuejie Chi</a>
                and Prof. <a href="https://www.cs.cmu.edu/~cga/">Chris Atkeson</a>. I received my BS from University of Wisconsin-Madison in Computer Science, 
                and worked as an undergraduate researcher at <a href="https://skunkworks.engr.wisc.edu/">Informatics Skunkworks</a>, under the 
                supervision of Prof. <a href="https://directory.engr.wisc.edu/mse/faculty/morgan_dane">Dane Morgan</a>.
                <br>
                <br>

                <p style="text-align:center">
                <a href="mailto:yiqiw2@andrew.cmu.edu">Email</a> &nbsp/&nbsp 
                <a href="data/Resume_Yiqi_Wang_new.pdf">Resume (Feb 2024)</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=DJ1vgWMAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yiqiwang8177">Github</a> &nbsp/&nbsp
  
                  </p>
                </p>
            </td>
            <br>
            <td width="30%" valign="right">
                <a href="images/profile.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>

        </tr>
        </table>
    </td>
    </tr>
    </table>

    <br>
    <br>


    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
            <h2> Research Interests </h2>
            <hr>
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                <tr>
                    <td width="100%" valign="middle">
                        <p>
                            I am interested in scalable skill acquisition for embodied agent like robots, which may enable robots mastering many skills and 
                            are capable of operating along with humans. I'm trying to accomplish such a goal by developing learning paradigms
                            which enables us to help an agent to develop new skills by leveraing existing data (priors) from other agents, including agent with different
                            observation and action space. I'm particularly interested in asking robots to learn from humans, who
                            produces huge amount of data on social media, and online video plateform. 
              
                        </p>
                    </td>
                </tr>
            </table>
    
        </td>
        </tr>
    </table>
        
    <br>
    <br>

    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
            <h2> Projects </h2>

            <table width="800" align="center" border="0" cellspacing="0" cellpadding="0">
                <tr>
                    <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                        <div class="image-container">
                            <img src="images/DTd_concept.png" width="100%">
                        </div>
                    </td>
                    <td style="width:65%; vertical-align:middle">
                        <papertext>
                            <papertitle>A trajectory is worth three sentences: multimodal transformer for offline reinforcement learning</papertitle>
                            <br>
                        <strong>Yiqi Wang</strong>,
                        <a href="https://mxu34.github.io/"><author>Mengdi Xu</author></a>,
                        <a href="https://laixishi.github.io/"><author>Laixi Shi</author></a>,
                        <a href="https://users.ece.cmu.edu/~yuejiec/"><author>Yuejie Chi</author></a>,
                       
                        <br>
                        Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, (<strong>UAI</strong>), 2023
                        <br>
                        <a href="https://proceedings.mlr.press/v216/wang23d.html">[paper]</a>
                        <a href="https://github.com/yiqiwang8177/Official-codebase-for-Decision-Transducer/">[code]</a>
                        <a href="https://www.youtube.com/watch?v=-f3zd_2GWuE">[presentation]</a>
                        <div class="read-more-link" onclick="showPopup()">[read more]</div>

                        <div class="popup" id="popup">
                            <div class="popup-content">
                            <span class="close-btn" onclick="closePopup()">&times;</span>
                            <ul>
                                <li>This paper formulates offline RL via Transformer as a multimodal sequence modeling problem.</li>
                                <li>Multimodal quantification process is proposed to discover importance of modality in offline RL (shown in the 3x3 table).</li>
                                <li>Decision Transducer (DTd) is proposed as a multimodal transformer, by hierachically leveraging importance of modality discovered by the quantification process.</li>
                                <li>DTd is data-efficient (achieves prior arts with 50% gradient steps) and competitive in performance (surpass all transfromer baselines on average across 9 test suits from D4RL)</li>
                            </ul>
                            </div>
                        </div>
                          
                        <script>
                            function showPopup() {
                            document.getElementById('popup').style.display = 'block';
                            }

                            function closePopup() {
                            document.getElementById('popup').style.display = 'none';
                            }
                            
                        </script>

                        

                        </papertext>
                    </td>
                </tr>
            </table>
        
            <br>

            <table width="800" align="center" border="0" cellspacing="0" cellpadding="0">
                <tr>
                    <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                        <div class="image-container">
                            <img src="images/ASR_meets_LM.png" width="100%">
                        </div>
                    </td>
                    <td style="width:65%; vertical-align:middle">
                        <papertext>
                            <papertitle>Automatic Speech Recognition Meets Language Modeling</papertitle>
                            <br>
                        <strong>Yiqi Wang</strong>,
                        <a><author>Jianyu Mao</author></a>,
                        <a ><author>Aditya Rathod</author></a>,
                       

                        <a href="https://github.com/yiqiwang8177/speechbrain/tree/develop/recipes/LibriSpeech/ASR_LM_Cotrain/">[code]</a>

                        <div class="read-more-link" onclick="showPopup2()">[read more]</div>

                        <div class="popup" id="popup2">
                            <div class="popup-content2">
                            <span class="close-btn2" onclick="closePopup2()">&times;</span>
                            <ul>
                                <li>We try to explore the following: could we benefit multimodal task (ASR:audio -> text) by unimodal pretraining (LM: text-only)?.</li>
                                <li>We proposesd a multiway multimodal transformer (each transformer blocks has 2 FFNs), in order to instigate the problem above.</li>
                                <li>After LM pretraining, we add an extra trainable FFN to each transformer block to accomodate the audio modality during the ASR, called audio expert. 
                                    The original FFN inherit from LM pretrianing is called text expert. The self-attn is also inherit from LM pretrianing.
                                </li>  
                                <li>We observed that naively feed a new modality to the unimodal pretrained modal with a newly added trainable FFN to each block introduce training instability.</li>
                                <li>Our hypothesis is that the gap in modality representation (audio VS. text) introduce this issue. We're now instigating grounding technique to fix thiss issue.</li>
                            </ul>
                            </div>
                        </div>
                          
                        <script>
                            function showPopup2() {
                            document.getElementById('popup2').style.display = 'block';
                            }

                            function closePopup2() {
                            document.getElementById('popup2').style.display = 'none';
                            }
                            
                        </script>

                        </papertext>
                    </td>
                </tr>
            </table>


            <!-- <table width="800" align="center" border="0" cellspacing="0" cellpadding="0">
                <tr>
                    <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                        <div class="image-container">
                            <img src="images/Text-cond_Video-Video.png" width="100%">
                        </div>
                    </td>
                    <td style="width:65%; vertical-align:middle">
                        <papertext>
                            <papertitle>Scalable Robot Skill Acquisition with K references</papertitle>
                            <br>
                        <strong>Yiqi Wang</strong>, -->
                        <!-- <a href="http://mrinal.verghese.org/"><author>Mrinal Verghese</author></a>,
                        <a href="https://www.cs.cmu.edu/~cga/"><author>Chris Atkeson</author></a>,
                       
                        <br>
                        In progresss
                        <br>

                        <div class="read-more-link" onclick="showPopup3()">[read more]</div> -->

                        <!-- <div class="popup" id="popup3">
                            <div class="popup-content3">
                            <span class="close-btn3" onclick="closePopup3()">&times;</span>
                            <ul>
                                <li>Could we benefit robot skill acquisition by transferring skill demonstrated by humans on Internet via VLMs?</li>
                                <li>I observed that VLMs (text-video model) is insufficient to distinguish nuances in text descriptions. Thus, insufficient to supervise robot skill learning.</li>
                                <ul>
                                    <li>Pouring wine from bottle involves gental movement of hand.</li>
                                    <li>Pouring sticky sauce from bottle involves quickly flipping the container and shaking.</li>
                                    <li>Rewards similar in scale are provided in both cases, which is unexpected.</li>
                                </ul> -->

                                <!-- <li>Since direct cross-modal alignment between video and text are demanding, I'm exploring video-video paradigm right now (illustrated in the figure).</li> 
                            </ul>
                            </div>
                        </div>
                          
                        <script>
                            function showPopup3() {
                            document.getElementById('popup3').style.display = 'block';
                            } -->

    <!-- //                         function closePopup3() {
    //                         document.getElementById('popup3').style.display = 'none';
    //                         }
                            
    //                     </script>

    //                     </papertext>
    //                 </td>
    //             </tr>
    //         </table>

    //     </tr>
    //     </tr>
    //  </table>       -->
   
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
    
            <p align="right">
                Website template from <a href="https://github.com/jonbarron/website">Jon Barron</a>.
            </p>
            
        </td>
        </tr>
    </table>
    <br>


</body>

</html>